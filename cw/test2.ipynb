{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6700 (pos=635, neg=6065), pos_weight=9.55\n",
      "Epoch 1/25 | Train Loss: 1.2553, Train Acc: 90.48% | Val Loss: 1.2485, Val Acc: 81.37%\n",
      "Epoch 2/25 | Train Loss: 1.2372, Train Acc: 89.16% | Val Loss: 1.2385, Val Acc: 77.25%\n",
      "Epoch 3/25 | Train Loss: 1.2140, Train Acc: 86.13% | Val Loss: 1.2274, Val Acc: 81.25%\n",
      "Epoch 4/25 | Train Loss: 1.1856, Train Acc: 87.96% | Val Loss: 1.2166, Val Acc: 82.81%\n",
      "Epoch 5/25 | Train Loss: 1.1558, Train Acc: 89.54% | Val Loss: 1.2058, Val Acc: 82.99%\n",
      "Epoch 6/25 | Train Loss: 1.1252, Train Acc: 89.85% | Val Loss: 1.1943, Val Acc: 82.03%\n",
      "Epoch 7/25 | Train Loss: 1.0936, Train Acc: 89.22% | Val Loss: 1.1827, Val Acc: 80.96%\n",
      "Epoch 8/25 | Train Loss: 1.0615, Train Acc: 88.64% | Val Loss: 1.1713, Val Acc: 80.42%\n",
      "Epoch 9/25 | Train Loss: 1.0293, Train Acc: 88.45% | Val Loss: 1.1607, Val Acc: 80.54%\n",
      "Epoch 10/25 | Train Loss: 0.9973, Train Acc: 88.45% | Val Loss: 1.1511, Val Acc: 80.90%\n",
      "Epoch 11/25 | Train Loss: 0.9656, Train Acc: 88.99% | Val Loss: 1.1427, Val Acc: 81.73%\n",
      "Epoch 12/25 | Train Loss: 0.9341, Train Acc: 89.52% | Val Loss: 1.1357, Val Acc: 82.03%\n",
      "Epoch 13/25 | Train Loss: 0.9031, Train Acc: 90.37% | Val Loss: 1.1300, Val Acc: 83.52%\n",
      "Epoch 14/25 | Train Loss: 0.8725, Train Acc: 91.12% | Val Loss: 1.1253, Val Acc: 84.60%\n",
      "Epoch 15/25 | Train Loss: 0.8426, Train Acc: 91.96% | Val Loss: 1.1216, Val Acc: 85.19%\n",
      "Epoch 16/25 | Train Loss: 0.8133, Train Acc: 92.49% | Val Loss: 1.1186, Val Acc: 85.85%\n",
      "Epoch 17/25 | Train Loss: 0.7847, Train Acc: 92.90% | Val Loss: 1.1161, Val Acc: 86.21%\n",
      "Epoch 18/25 | Train Loss: 0.7568, Train Acc: 93.19% | Val Loss: 1.1142, Val Acc: 86.81%\n",
      "Epoch 19/25 | Train Loss: 0.7296, Train Acc: 93.48% | Val Loss: 1.1128, Val Acc: 86.99%\n",
      "Epoch 20/25 | Train Loss: 0.7032, Train Acc: 93.66% | Val Loss: 1.1120, Val Acc: 87.16%\n",
      "Epoch 21/25 | Train Loss: 0.6775, Train Acc: 93.84% | Val Loss: 1.1121, Val Acc: 87.40%\n",
      "Epoch 22/25 | Train Loss: 0.6527, Train Acc: 94.10% | Val Loss: 1.1131, Val Acc: 87.52%\n",
      "Epoch 23/25 | Train Loss: 0.6286, Train Acc: 94.36% | Val Loss: 1.1152, Val Acc: 87.52%\n",
      "Epoch 24/25 | Train Loss: 0.6053, Train Acc: 94.52% | Val Loss: 1.1184, Val Acc: 87.70%\n",
      "Epoch 25/25 | Train Loss: 0.5828, Train Acc: 94.70% | Val Loss: 1.1226, Val Acc: 87.70%\n",
      "\n",
      "=== Test Set Evaluation ===\n",
      "Test Accuracy: 0.8419, F1: 0.3548\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "BoW + MLP Example with pos_weight Adjustment for Imbalanced Data\n",
    "----------------------------------------------------------------\n",
    "1) Reads the 'dontpatronizeme_pcl.tsv' dataset using DontPatronizeMe class.\n",
    "2) Splits the data into train/validation/test sets (the dev set is treated as a test set).\n",
    "3) Uses CountVectorizer to convert paragraphs into Bag-of-Words (BoW) feature vectors.\n",
    "4) Defines a simple MLP (two-layer feed-forward network) in PyTorch.\n",
    "5) Computes pos_weight for BCEWithLogitsLoss to account for class imbalance.\n",
    "6) Trains the MLP, evaluates it on validation and finally on test (dev) set.\n",
    "\n",
    "Please ensure you have installed:\n",
    "- pandas, numpy, scikit-learn\n",
    "- PyTorch\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DontPatronizeMe:\n",
    "    \"\"\"\n",
    "    A simple data-loading class that reads 'dontpatronizeme_pcl.tsv' and \n",
    "    converts original labels (0,1,2,3,4) into binary (0/1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _train_path, _test_path):\n",
    "        self.train_path = _train_path\n",
    "        self.test_path = _test_path\n",
    "        self.train_task1_df = None\n",
    "\n",
    "    def load_task1(self):\n",
    "        \"\"\"\n",
    "        Reads the TSV file starting from the 5th line, \n",
    "        splits each line by tab, and assigns a binary label:\n",
    "        - '0' or '1' => label=0\n",
    "        - '2', '3', or '4' => label=1\n",
    "        Stores the result in self.train_task1_df.\n",
    "        \"\"\"\n",
    "        rows=[]\n",
    "        with open(self.train_path) as f:\n",
    "            # Skip the first 4 lines, which are just descriptions\n",
    "            for line in f.readlines()[4:]:\n",
    "                fields = line.strip().split('\\t')\n",
    "                par_id = fields[0]\n",
    "                art_id = fields[1]\n",
    "                keyword = fields[2]\n",
    "                country = fields[3]\n",
    "                text_ = fields[4]\n",
    "                orig_label = fields[-1]\n",
    "                \n",
    "                # Convert original label to binary\n",
    "                if orig_label in ['0','1']:\n",
    "                    lbin = 0\n",
    "                else:\n",
    "                    lbin = 1\n",
    "                rows.append({\n",
    "                    'par_id': par_id,\n",
    "                    'art_id': art_id,\n",
    "                    'keyword': keyword,\n",
    "                    'country': country,\n",
    "                    'text': text_,\n",
    "                    'label': lbin,\n",
    "                    'orig_label': orig_label\n",
    "                })\n",
    "        df = pd.DataFrame(rows, columns=['par_id','art_id','keyword','country','text','label','orig_label'])\n",
    "        self.train_task1_df = df\n",
    "\n",
    "def get_test(user):\n",
    "    \"\"\"\n",
    "    Loads the entire dataset via DontPatronizeMe, then filters by 'dev_semeval_parids-labels.csv' \n",
    "    to extract the dev set, which we treat as the test set here.\n",
    "    \"\"\"\n",
    "    _train_path = f'{user}/cw/dontpatronizeme_pcl.tsv'\n",
    "    _test_path = f'{user}/cw/task4_test.tsv'\n",
    "    \n",
    "    dpm = DontPatronizeMe(_train_path, _test_path)\n",
    "    dpm.load_task1()\n",
    "    \n",
    "    train_data = dpm.train_task1_df\n",
    "    train_data[\"par_id\"] = train_data[\"par_id\"].astype(str)\n",
    "    \n",
    "    dev_parids = pd.read_csv(\"dev_semeval_parids-labels.csv\")\n",
    "    dev_parids[\"par_id\"] = dev_parids[\"par_id\"].astype(str)\n",
    "    dev_parid_list = dev_parids[\"par_id\"].unique()\n",
    "    \n",
    "    dev_data = train_data[train_data[\"par_id\"].isin(dev_parid_list)]\n",
    "    return dev_data\n",
    "\n",
    "def get_train(user):\n",
    "    \"\"\"\n",
    "    Loads the dataset via DontPatronizeMe, then filters by 'train_semeval_parids-labels.csv'\n",
    "    to extract the train set.\n",
    "    \"\"\"\n",
    "    _train_path = f'{user}/cw/dontpatronizeme_pcl.tsv'\n",
    "    _test_path = f'{user}/cw/task4_test.tsv'\n",
    "    \n",
    "    dpm = DontPatronizeMe(_train_path, _test_path)\n",
    "    dpm.load_task1()\n",
    "    \n",
    "    train_data = dpm.train_task1_df\n",
    "    train_data[\"par_id\"] = train_data[\"par_id\"].astype(str)\n",
    "    \n",
    "    train_parids = pd.read_csv(\"train_semeval_parids-labels.csv\")\n",
    "    train_parids[\"par_id\"] = train_parids[\"par_id\"].astype(str)\n",
    "    train_parid_list = train_parids[\"par_id\"].unique()\n",
    "    \n",
    "    train_filtered_data = train_data[train_data[\"par_id\"].isin(train_parid_list)]\n",
    "    return train_filtered_data\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 2-layer feed-forward network (MLP) for binary classification:\n",
    "    [Linear -> ReLU -> Linear -> output logits]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main workflow:\n",
    "    1) Load train data (and dev set) using the same splits as in the DeBERTa code.\n",
    "    2) Perform an 80-20 train-validation split on the train set.\n",
    "    3) Transform text to BoW features (CountVectorizer).\n",
    "    4) Define the MLP model in PyTorch and compute pos_weight for BCEWithLogitsLoss\n",
    "       to handle class imbalance.\n",
    "    5) Train for a fixed number of epochs (e.g., 30).\n",
    "    6) Evaluate on both validation set and final test (dev) set, \n",
    "       printing Accuracy and F1 scores.\n",
    "    \"\"\"\n",
    "\n",
    "    user = \"/vol/bitbucket/cx720/cw/nlp/70016-Natural-Language-Processing/\"\n",
    "    \n",
    "    # 1) Load train and test data\n",
    "    train_data = get_train(user)\n",
    "    test_data  = get_test(user)\n",
    "\n",
    "    # 2) Split train_data into train and validation sets\n",
    "    train_train_data, train_val_data = train_test_split(\n",
    "        train_data, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=train_data['label']\n",
    "    )\n",
    "\n",
    "    # 3) Create BoW features with CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(train_train_data['text'])\n",
    "\n",
    "    def bow_transform(df):\n",
    "        # Convert sparse matrix to a dense NumPy array\n",
    "        return vectorizer.transform(df['text']).toarray()\n",
    "\n",
    "    X_train = bow_transform(train_train_data)\n",
    "    y_train = train_train_data['label'].values\n",
    "\n",
    "    X_val = bow_transform(train_val_data)\n",
    "    y_val = train_val_data['label'].values\n",
    "\n",
    "    X_test = bow_transform(test_data)\n",
    "    y_test = test_data['label'].values\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "    X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "    y_val_t   = torch.tensor(y_val,   dtype=torch.float32).view(-1,1)\n",
    "\n",
    "    X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "    y_test_t  = torch.tensor(y_test,  dtype=torch.float32).view(-1,1)\n",
    "\n",
    "    # 4) Define MLP model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = MLP(input_dim=input_dim, hidden_dim=64)\n",
    "\n",
    "    # Calculate pos_weight to address class imbalance\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    if pos_count == 0:\n",
    "        # If no positive samples at all, fallback\n",
    "        pos_weight_value = 1.0\n",
    "    else:\n",
    "        pos_weight_value = neg_count / pos_count\n",
    "    print(f\"Training samples: {len(y_train)} (pos={pos_count}, neg={neg_count}), \"\n",
    "          f\"pos_weight={pos_weight_value:.2f}\")\n",
    "\n",
    "    # Use BCEWithLogitsLoss with pos_weight\n",
    "    criterion = nn.BCEWithLogitsLoss(\n",
    "        pos_weight=torch.tensor([pos_weight_value], dtype=torch.float32)\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # 5) Training loop\n",
    "    epochs = 25\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(X_train_t)\n",
    "        loss = criterion(logits, y_train_t)\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on the training set\n",
    "        with torch.no_grad():\n",
    "            train_preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            train_acc = (train_preds == y_train_t).float().mean().item()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_val_t)\n",
    "            val_loss = criterion(val_logits, y_val_t)\n",
    "            val_preds = (torch.sigmoid(val_logits) >= 0.5).float()\n",
    "            val_acc = (val_preds == y_val_t).float().mean().item()\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "              f\"Train Loss: {loss.item():.4f}, Train Acc: {train_acc*100:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # 6) Final evaluation on the test (dev) set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(X_test_t)\n",
    "        test_probs = torch.sigmoid(test_logits).view(-1).cpu().numpy()\n",
    "        test_preds = (test_probs >= 0.5).astype(int)\n",
    "        y_test_1d  = y_test_t.view(-1).cpu().numpy()\n",
    "\n",
    "    test_acc = accuracy_score(y_test_1d, test_preds)\n",
    "    test_f1  = f1_score(y_test_1d, test_preds)\n",
    "    print(\"\\n=== Test Set Evaluation ===\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
