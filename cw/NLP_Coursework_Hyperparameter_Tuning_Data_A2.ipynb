{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from transformers import DebertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "user=\"/vol/bitbucket/cx720/cw/nlp/70016-Natural-Language-Processing/\"\n",
    "# user=\"/vol/bitbucket/xz223/dlenv/NLP/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DontPatronizeMe:\n",
    "\n",
    "\tdef __init__(self, _train_path, _test_path):\n",
    "\n",
    "\t\tself.train_path = _train_path\n",
    "\t\tself.test_path = _test_path\n",
    "\t\tself.train_task1_df = None\n",
    "\t\tself.test_set_df = None\n",
    "\n",
    "\tdef load_task1(self):\n",
    "\t\t\"\"\"\n",
    "\t\tLoad task 1 training set and convert the tags into binary labels. \n",
    "\t\tParagraphs with original labels of 0 or 1 are considered to be negative examples of PCL and will have the label 0 = negative.\n",
    "\t\tParagraphs with original labels of 2, 3 or 4 are considered to be positive examples of PCL and will have the label 1 = positive.\n",
    "\t\tIt returns a pandas dataframe with paragraphs and labels.\n",
    "\t\t\"\"\"\n",
    "\t\trows=[]\n",
    "\t\twith open(self.train_path) as f:\n",
    "\t\t\tfor line in f.readlines()[4:]:\n",
    "\t\t\t\tpar_id=line.strip().split('\\t')[0]\n",
    "\t\t\t\tart_id = line.strip().split('\\t')[1]\n",
    "\t\t\t\tkeyword=line.strip().split('\\t')[2]\n",
    "\t\t\t\tcountry=line.strip().split('\\t')[3]\n",
    "\t\t\t\tt=line.strip().split('\\t')[4]#.lower()\n",
    "\t\t\t\tl=line.strip().split('\\t')[-1]\n",
    "\t\t\t\tif l=='0' or l=='1':\n",
    "\t\t\t\t\tlbin=0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlbin=1\n",
    "\t\t\t\trows.append(\n",
    "\t\t\t\t\t{'par_id':par_id,\n",
    "\t\t\t\t\t'art_id':art_id,\n",
    "\t\t\t\t\t'keyword':keyword,\n",
    "\t\t\t\t\t'country':country,\n",
    "\t\t\t\t\t'text':t, \n",
    "\t\t\t\t\t'label':lbin, \n",
    "\t\t\t\t\t'orig_label':l\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t\t)\n",
    "\t\tdf=pd.DataFrame(rows, columns=['par_id', 'art_id', 'keyword', 'country', 'text', 'label', 'orig_label']) \n",
    "\t\tself.train_task1_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(user):\n",
    "    _train_path = f'{user}/cw/dontpatronizeme_pcl.tsv'\n",
    "    _test_path = f'{user}/cw/task4_test.tsv'\n",
    "    \n",
    "    dpm = DontPatronizeMe(_train_path, _test_path)\n",
    "    dpm.load_task1()\n",
    "    \n",
    "    train_data = dpm.train_task1_df\n",
    "    train_data[\"par_id\"] = train_data[\"par_id\"].astype(str)\n",
    "    \n",
    "    dev_parids = pd.read_csv(\"dev_semeval_parids-labels.csv\")\n",
    "    dev_parids[\"par_id\"] = dev_parids[\"par_id\"].astype(str)\n",
    "    dev_parid_list = dev_parids[\"par_id\"].unique()\n",
    "    dev_data = train_data[train_data[\"par_id\"].isin(dev_parid_list)]\n",
    "    return dev_data\n",
    "\n",
    "def get_train(user):\n",
    "    _train_path = f'{user}/cw/dontpatronizeme_pcl.tsv'\n",
    "    _test_path = f'{user}/cw/task4_test.tsv'\n",
    "    \n",
    "    dpm = DontPatronizeMe(_train_path, _test_path)\n",
    "    dpm.load_task1()\n",
    "    \n",
    "    train_data = dpm.train_task1_df\n",
    "    train_data[\"par_id\"] = train_data[\"par_id\"].astype(str)\n",
    "    \n",
    "    train_parids = pd.read_csv(\"train_semeval_parids-labels.csv\")\n",
    "    train_parids[\"par_id\"] = train_parids[\"par_id\"].astype(str)\n",
    "    train_parid_list = train_parids[\"par_id\"].unique()\n",
    "    train_filtered_data = train_data[train_data[\"par_id\"].isin(train_parid_list)]\n",
    "    return train_filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_train(user)\n",
    "test_data = get_test(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "train_train_data, train_val_data = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "augmenter = naw.SynonymAug(\n",
    "    aug_src=\"wordnet\",             \n",
    "    aug_p=0.15,                  \n",
    "    stopwords=stopwords.words(\"english\")  \n",
    ")\n",
    "\n",
    "train_data_positive = train_train_data.loc[train_train_data[\"label\"]]\n",
    "\n",
    "augmented_data_all = []\n",
    "for i in range(len(train_data_positive)):\n",
    "    original_sentence = train_data_positive.iloc[i][\"text\"]\n",
    "    for j in range(5): \n",
    "        augmented_sentence = augmenter.augment(original_sentence)\n",
    "        augmented_data = {\n",
    "            \"keyword\": train_data_positive.iloc[i][\"keyword\"],\n",
    "            \"text\": augmented_sentence[0],\n",
    "            \"label\": True\n",
    "        }\n",
    "        augmented_data_all.append(augmented_data)\n",
    "\n",
    "\n",
    "augmented_data_df = pd.DataFrame.from_records(augmented_data_all)\n",
    "\n",
    "train_train_data = pd.concat([train_train_data, augmented_data_df], ignore_index=True)\n",
    "train_train_data = train_train_data.sample(frac=1, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\", do_lower_case=False)\n",
    "\n",
    "def encode_data(data, tokenizer, max_length=512):\n",
    "    return tokenizer(data['text'].tolist(), return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "train_train_encodings = encode_data(train_train_data, tokenizer)\n",
    "train_val_encodings = encode_data(train_val_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train_dataset = PCLDataset(train_train_encodings, list(train_train_data[\"label\"]))\n",
    "train_val_dataset = PCLDataset(train_val_encodings, list(train_val_data[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(input):\n",
    "    y_pred = np.argmax(input.predictions, axis=1)\n",
    "    y_true = input.label_ids\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1score = f1_score(y_true, y_pred)\n",
    "    return {'accuracy': accuracy, 'f1 score': f1score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "test = [2]\n",
    "\n",
    "for t in test:\n",
    "    # positive da\n",
    "    model_name = f\"deberta_DA_{t}\"\n",
    "    print(\"Start \" + model_name)\n",
    "    \n",
    "    num_labels = 2  \n",
    "    model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=num_labels)\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # setting training parameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./Deberta/pcl_deberta_model_base',\n",
    "        learning_rate=1e-5,  \n",
    "        weight_decay=0.05,\n",
    "        num_train_epochs=10,  # 10 epochs\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_steps=10,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "    \n",
    "    \n",
    "    # create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_train_dataset,\n",
    "        eval_dataset=train_val_dataset,\n",
    "        compute_metrics=compute_metrics  \n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    with open(f'./outputs/{model_name}.txt', \"w\") as f:\n",
    "        for log in trainer.state.log_history:\n",
    "            if not any(key.startswith(\"loss\") for key in log):\n",
    "                f.write(str(log) + \"\\n\")\n",
    "    \n",
    "    model.save_pretrained(f'./models/{model_name}')\n",
    "    tokenizer.save_pretrained(f'./tokenizers/{model_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
