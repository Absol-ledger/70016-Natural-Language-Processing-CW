{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Validation Set ===\n",
      "Accuracy: 0.9110, F1: 0.3134\n",
      "=== Test Set ===\n",
      "Accuracy: 0.8968, F1: 0.2286\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Logistic Regression + Bag-of-Words (BoW) Example\n",
    "-----------------------------------------------\n",
    "This script demonstrates:\n",
    "1) How to load the 'dontpatronizeme_pcl.tsv' data using the same methods (get_train, get_test)\n",
    "   as in the DeBERTa code.\n",
    "2) How to use scikit-learn's CountVectorizer to convert text into BoW features.\n",
    "3) How to train and evaluate a Logistic Regression classifier on the task of classifying paragraphs\n",
    "   into two classes: label=0 (non-patronizing) vs. label=1 (patronizing).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "class DontPatronizeMe:\n",
    "    \"\"\"\n",
    "    A simple class that reads the 'dontpatronizeme_pcl.tsv' file.\n",
    "    It extracts paragraphs and converts original labels {0,1,2,3,4}\n",
    "    into a binary label (0 = non-patronizing, 1 = patronizing).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _train_path, _test_path):\n",
    "        self.train_path = _train_path\n",
    "        self.test_path = _test_path\n",
    "        self.train_task1_df = None\n",
    "\n",
    "    def load_task1(self):\n",
    "        \"\"\"\n",
    "        Reads the tsv file starting from the 5th line (skipping headers).\n",
    "        Each line is split by tab into multiple fields.\n",
    "        If orig_label is 0 or 1, we set label=0; otherwise (2,3,4) => label=1.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        with open(self.train_path) as f:\n",
    "            # Skip the first 4 lines which contain only description\n",
    "            for line in f.readlines()[4:]:\n",
    "                fields = line.strip().split('\\t')\n",
    "                par_id = fields[0]\n",
    "                art_id = fields[1]\n",
    "                keyword = fields[2]\n",
    "                country = fields[3]\n",
    "                text_ = fields[4]\n",
    "                orig_label = fields[-1]\n",
    "\n",
    "                # Convert original label to binary\n",
    "                if orig_label in ['0','1']:\n",
    "                    lbin = 0\n",
    "                else:\n",
    "                    lbin = 1\n",
    "\n",
    "                rows.append({\n",
    "                    'par_id': par_id,\n",
    "                    'art_id': art_id,\n",
    "                    'keyword': keyword,\n",
    "                    'country': country,\n",
    "                    'text': text_,\n",
    "                    'label': lbin,\n",
    "                    'orig_label': orig_label\n",
    "                })\n",
    "        df = pd.DataFrame(\n",
    "            rows, \n",
    "            columns=['par_id','art_id','keyword','country','text','label','orig_label']\n",
    "        )\n",
    "        self.train_task1_df = df\n",
    "\n",
    "def get_test(user):\n",
    "    \"\"\"\n",
    "    Loads the entire dataset from 'dontpatronizeme_pcl.tsv' via DontPatronizeMe,\n",
    "    then filters to keep only paragraphs in 'dev_semeval_parids-labels.csv',\n",
    "    effectively creating a Dev/Test set for evaluation.\n",
    "    \"\"\"\n",
    "    _train_path = f'{user}/cw/dontpatronizeme_pcl.tsv'\n",
    "    _test_path = f'{user}/cw/task4_test.tsv'\n",
    "    \n",
    "    dpm = DontPatronizeMe(_train_path, _test_path)\n",
    "    dpm.load_task1()\n",
    "    \n",
    "    train_data = dpm.train_task1_df\n",
    "    train_data[\"par_id\"] = train_data[\"par_id\"].astype(str)\n",
    "    \n",
    "    # Read the dev set par_ids\n",
    "    dev_parids = pd.read_csv(\"dev_semeval_parids-labels.csv\")\n",
    "    dev_parids[\"par_id\"] = dev_parids[\"par_id\"].astype(str)\n",
    "    dev_parid_list = dev_parids[\"par_id\"].unique()\n",
    "    \n",
    "    # Filter only paragraphs with par_id in dev set\n",
    "    dev_data = train_data[train_data[\"par_id\"].isin(dev_parid_list)]\n",
    "    return dev_data\n",
    "\n",
    "def get_train(user):\n",
    "    \"\"\"\n",
    "    Similar to get_test, but keeps only paragraphs in 'train_semeval_parids-labels.csv',\n",
    "    producing the training set.\n",
    "    \"\"\"\n",
    "    _train_path = f'{user}/cw/dontpatronizeme_pcl.tsv'\n",
    "    _test_path = f'{user}/cw/task4_test.tsv'\n",
    "    \n",
    "    dpm = DontPatronizeMe(_train_path, _test_path)\n",
    "    dpm.load_task1()\n",
    "    \n",
    "    train_data = dpm.train_task1_df\n",
    "    train_data[\"par_id\"] = train_data[\"par_id\"].astype(str)\n",
    "    \n",
    "    train_parids = pd.read_csv(\"train_semeval_parids-labels.csv\")\n",
    "    train_parids[\"par_id\"] = train_parids[\"par_id\"].astype(str)\n",
    "    train_parid_list = train_parids[\"par_id\"].unique()\n",
    "    \n",
    "    # Filter the main data to keep only paragraphs in the train split\n",
    "    train_filtered_data = train_data[train_data[\"par_id\"].isin(train_parid_list)]\n",
    "    return train_filtered_data\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main workflow:\n",
    "    1) Load the train/dev data from the same dataset used by the DeBERTa example.\n",
    "    2) Split train_data into train_train_data and train_val_data.\n",
    "    3) Use CountVectorizer to create bag-of-words (BoW) features.\n",
    "    4) Train a Logistic Regression classifier.\n",
    "    5) Evaluate on both validation set and dev (test) set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Adjust 'user' path accordingly for your environment\n",
    "    user = \"/vol/bitbucket/cx720/cw/nlp/70016-Natural-Language-Processing/\"\n",
    "    \n",
    "    # 1) Load train and test (dev) data\n",
    "    train_data = get_train(user)   # entire train set\n",
    "    test_data  = get_test(user)    # dev set\n",
    "\n",
    "    # 2) Split train_data into train/val\n",
    "    train_train_data, train_val_data = train_test_split(\n",
    "        train_data, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=train_data['label']\n",
    "    )\n",
    "\n",
    "    # 3) Fit CountVectorizer on training set\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(train_train_data[\"text\"])\n",
    "\n",
    "    # Transform text into BoW features\n",
    "    def transform_text(df):\n",
    "        # Transform returns a sparse matrix; we convert it to a dense array\n",
    "        return vectorizer.transform(df['text']).toarray()\n",
    "\n",
    "    X_train = transform_text(train_train_data)\n",
    "    y_train = train_train_data[\"label\"].values\n",
    "\n",
    "    X_val = transform_text(train_val_data)\n",
    "    y_val = train_val_data[\"label\"].values\n",
    "\n",
    "    X_test = transform_text(test_data)\n",
    "    y_test = test_data[\"label\"].values\n",
    "\n",
    "    # 4) Train a Logistic Regression model on the training set\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    val_preds = clf.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    val_f1  = f1_score(y_val, val_preds)\n",
    "\n",
    "    print(\"=== Validation Set ===\")\n",
    "    print(f\"Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # 5) Evaluate on the test (dev) set\n",
    "    test_preds = clf.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    test_f1 = f1_score(y_test, test_preds)\n",
    "\n",
    "    print(\"=== Test Set ===\")\n",
    "    print(f\"Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
